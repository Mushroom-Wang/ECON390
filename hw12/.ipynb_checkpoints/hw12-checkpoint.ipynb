{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an infinitely repeated duopoly market with stochastic demand. Specifically, firms 1 and 2 interact over time $t \\in\\{1, \\ldots, \\infty\\}$ with the demand in each time period being either high (H) or low (L). If the demand is $\\mathrm{H}$, then the inverse demand function is given by $p_{H}=120-q_{1}-q_{2} ;$ if the demand is $L,$ then the inverse demand function is given by $p_{L}=60-q_{1}-q_{2}$ (where $q_{1}$ is quantity chosen by firm $1,$ and $q_{2}$ is the quantity chosen by firm 2). Furthermore, suppose that firms discount the future according to the discount rate of $\\beta=.99$ and the demand evolves according to the transition probability matrix $M=\\left[\\begin{array}{rr}.8 & .2 \\\\ .3 & .7\\end{array}\\right] .$ For example, the probability that the demand in period $t+1$ is $\\mathrm{H}$ given that the demand in period $t$ is $\\mathrm{H}$ is . $8 .$ For simplicity, suppose that each firm produces at zero marginal cost.\n",
    "\n",
    "a) Suppose that firm 1 's policy is to produce 30 in every period. Determine the value function and the optimal policy functions for firm $2 .$ Hints: there are two possible states to consider: $(\\mathrm{H})$ and $(\\mathrm{L}) .$ Your goal is to find the value of being in each state and the optimal decision by firm 2 .\n",
    "\n",
    "b) Suppose that firm 1 's policy is to produce 25 in every round as long neither firm exceeded 35 in any round in the past. If either of the firms exceeded $35,$ firm 1 will produce $40 .$ Determine the value and the policy function for firm $2 .$ Hint: there are four possible states to consider: (H, neither firm exceeded 35), (H, one of the firms exceeded\n",
    "35), (L, neither firm exceeded exceeded 35), (L, one of the firms exceeded 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "M = np.array([[0.8, 0.2], [0.3, 0.7]])\n",
    "\n",
    "p_h = lambda q1, q2: 120 - q1 - q2\n",
    "p_l = lambda q1, q2: 60 - q1 - q2\n",
    "\n",
    "beta = 0.99\n",
    "\n",
    "H = 0\n",
    "L = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a\n",
    "\n",
    "For firm 1, the policy is \n",
    "$\n",
    "\\pi_1(a|s) = 30\n",
    "$.\n",
    "Given any state $s$, the output action $a$ of $\\pi_1$ is 30. \n",
    "\n",
    "\n",
    "For firm 2, the state space is $S = \\{H, L\\}$  \n",
    "The action space is $A = \\{a | a \\in \\mathbb{R} \\land 0 \\leq a < 90\\}$  \n",
    "We want to get the policy of firm 2, $\\pi_2$, with value iteration.  \n",
    "\n",
    "Value function for $\\pi_2$ is \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V(s) &= \\underset{a}{max} \\sum_{s' \\in S} \\mathcal{T} (s, a, s') (R(s, a, s') + \\beta V(s'))\\\\\n",
    "     % &= \\underset{a}{max} \\sum_{s' \\in S} M (R(s, a, s') + \\beta V(s'))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given current state $s$ and the action going to take $a$, $\\mathcal{T}$ returns the probability of which the ongoing state is $s'$. $\\mathcal{T}$ can be gotten by querying $M$.   \n",
    "Similarly, $R$ returns the reward when taking action $a$ in state $s$ and arriving state $s'$.   \n",
    "\n",
    "Once we get the converged (optimal) value function $V^*$, we have $$\\pi_2(a|s) = \\underset{a}{argmax} \\sum_{s' \\in S} \\mathcal{T} (s, a, s') (R(s, a, s') + \\beta V^*(s'))$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, the action will not affect the state transform, so we can simplify the $\\mathcal{T}(s, a, s')$ to $\\mathcal{T}(s, s')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(s0, s1):\n",
    "    return M[s0][s1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward function does not care about current state, so we can simplify the $R(s, a, s')$ to $R(a, s')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider revenue\n",
    "def rew(a, s1):\n",
    "    assert s1 in [0, 1]\n",
    "\n",
    "    if s1 == 0:  # next state is H\n",
    "        return p_h(30, a) * a\n",
    "    else:  # next state is L\n",
    "        return p_l(30, a) * a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value function and $\\pi_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.zeros(2)  # [0]: H, [1]: L\n",
    "\n",
    "def bf_argmax_a(fn, l, h):\n",
    "    # include upper boundary h\n",
    "    ind = np.arange(l, h + 1)\n",
    "    vs = fn(ind)\n",
    "    return l + np.argmax(vs), np.max(vs)\n",
    "\n",
    "def pi_2(s):\n",
    "    # TODO: v_a can be simplified as matrix product\n",
    "    v_a = lambda a: trans(s, 0) * (rew(a, 0) + beta * v[0]) + trans(s, 1) * (rew(a, 1) + beta * v[1])\n",
    "    return bf_argmax_a(v_a, 1, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(max_iter, delta):\n",
    "    for _ in range(max_iter):\n",
    "        new_v_0 = pi_2(0)[1]\n",
    "        new_v_1 = pi_2(1)[1]\n",
    "        new_v = np.array([new_v_0, new_v_1])\n",
    "        \n",
    "        global v\n",
    "        diff = np.max(new_v - v)\n",
    "        if diff <= delta: \n",
    "            break\n",
    "        \n",
    "        v = new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration(10000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If current state is H, firm 2 should produce: 39\n",
      "If current state is L, firm 2 should produce: 24\n"
     ]
    }
   ],
   "source": [
    "print(f\"If current state is H, firm 2 should produce: {pi_2(0)[0]}\")\n",
    "print(f\"If current state is L, firm 2 should produce: {pi_2(1)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V(s) &= \\underset{a}{max} \\sum_{s' \\in S} \\mathcal{T} (s, a, s') (R(s, a, s') + \\beta V(s'))\\\\\n",
    "     % &= \\underset{a}{max} \\sum_{s' \\in S} M (R(s, a, s') + \\beta V(s'))\n",
    "\\end{align}\n",
    "$$                   \n",
    "\n",
    "This question seems to be problematic. If we follow the hint, there are 4 states needed to be considered. However, the $\\mathcal{T}$ (a 4 x 4 matrix) for the 4 states is unknown. We do know the $M$, but, for example, we still cannot get the probability from $pr((H, q>35) \\rightarrow (H, q<35) | H)$. The reward for different current state $s$ is different. Thus we are not able to solve this problem without the full knowledges of $\\mathcal{T}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
